"""
Constants and Configuration File for the Batch-Aware RL Scheduler Simulation.

This file centralizes all the static parameters for the simulation,
environment, and RL agent, making it easy to tune and modify experiments.
"""

# ================================================================
# SECTION 1: PERFORMANCE PROFILE (The "Laws of Physics")
# ================================================================
# This dictionary was generated by profile_model.py.
# It defines the "physical law" of our Edge Node, specifying the
# processing time (in seconds) for a batch of a given size.
# Key: batch_size (int)
# Value: latency_in_seconds (float)
PERFORMANCE_PROFILE = {
    1: 0.0016378,
    2: 0.0015201,
    4: 0.0013424,
    8: 0.0016604,
    16: 0.0027963,
    32: 0.0053908,
    64: 0.0119798
}


# ================================================================
# SECTION 2: SIMULATION PARAMETERS (How the World Behaves)
# ================================================================
# Average time (in seconds) between the arrival of two consecutive tasks.
# For example, 0.05 means a new task arrives on average every 50ms.
# This simulates the load on the system. A smaller value means higher load.
TASK_ARRIVAL_INTERVAL_SECONDS = 0.05

# The lifespan (in seconds) of a task after it arrives.
# If a task is not completed by arrival_time + deadline, it is considered failed.
TASK_DEADLINE_SECONDS = 1.0

# How much simulation time (in seconds) passes in a single step of the environment.
# This defines the time resolution of our simulation.
SIM_STEP_SECONDS = 0.01 # 10 milliseconds


# ================================================================
# SECTION 3: RL AGENT & TRAINING PARAMETERS (The "Coach's" Plan)
# ================================================================
# Total number of timesteps to train the agent for.
TOTAL_TIMESTEPS = 100_000

# Learning rate for the RL agent's neural network.
LEARNING_RATE = 0.0001

# Discount factor for future rewards. A value close to 1 means the agent
# cares more about long-term rewards.
GAMMA = 0.99

# The size of the replay buffer, where the agent stores past experiences.
BUFFER_SIZE = 10_000

# How many steps of experience to collect before starting to train the model.
LEARNING_STARTS = 1000


# ================================================================
# SECTION 4: ENVIRONMENT DEFINITION (The "Game" Rules)
# ================================================================
# The number of features in our state vector.
# [queue_length, time_to_nearest_deadline, time_since_oldest_task]
NUM_STATE_FEATURES = 3

# --- Action Space Definitions ---
# Batch-aware action space:
# Action 0: WAIT (do not dispatch)
# Action 1-7: Dispatch batch of size [1, 2, 4, 8, 16, 32, 64]
ACTION_WAIT = 0
BATCH_SIZE_OPTIONS = [1, 2, 4, 8, 16, 32, 64]
NUM_ACTIONS = len(BATCH_SIZE_OPTIONS) + 1  # +1 for WAIT action


# ================================================================
# SECTION 5: ADAS-SPECIFIC PARAMETERS
# ================================================================
# Dataset path for real data flow environment
IMAGENETTE_PATH = "data/imagenette2"

# Device for neural network inference (real environment only)
INFERENCE_DEVICE = "cuda"  # or "cpu"

# Task arrival mode: "poisson" or "fixed_rate"
TASK_ARRIVAL_MODE = "poisson"  # Can be changed to "fixed_rate" for video stream simulation
FIXED_FRAME_RATE = 30  # FPS for fixed_rate mode (ADAS camera simulation)


# ================================================================
# SECTION 6: REWARD FUNCTION PARAMETERS
# ================================================================
# Reward for successfully completing a task before deadline
REWARD_TASK_SUCCESS = 2.0

# Penalty for missing a task's deadline
PENALTY_TASK_MISS = 10.0

# Penalty for a task expiring in the queue
PENALTY_QUEUE_EXPIRY = 15.0

# Latency penalty coefficient (to minimize overall system latency)
LATENCY_PENALTY_COEFF = 0.1

# Batch efficiency bonus coefficient (encourage batching)
BATCH_BONUS_COEFF = 0.5

# Invalid action penalties
PENALTY_EMPTY_QUEUE = 0.5
PENALTY_NODE_BUSY = 1.0
PENALTY_WAIT = 0.01  # Small penalty for waiting