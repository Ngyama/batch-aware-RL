"""
Constants and Configuration File for the Batch-Aware RL Scheduler Simulation.

This file centralizes all the static parameters for the simulation,
environment, and RL agent, making it easy to tune and modify experiments.
"""

# ================================================================
# SECTION 1: PERFORMANCE PROFILE (The "Laws of Physics")
# ================================================================
# This dictionary was generated by profile_model.py.
# It defines the "physical law" of our Edge Node, specifying the
# processing time (in seconds) for a batch of a given size.
# Key: batch_size (int)
# Value: latency_in_seconds (float)
PERFORMANCE_PROFILE = {
    1: 0.0016378,
    2: 0.0015201,
    4: 0.0013424,
    8: 0.0016604,
    16: 0.0027963,
    32: 0.0053908,
    64: 0.0119798
}


# ================================================================
# SECTION 2: SIMULATION PARAMETERS (How the World Behaves)
# ================================================================
# Average time (in seconds) between the arrival of two consecutive tasks.
# For example, 0.05 means a new task arrives on average every 50ms.
# This simulates the load on the system. A smaller value means higher load.

# --- LOW LOAD SCENARIO (Original Configuration) ---
# Suitable for: sparse task arrivals, baseline testing
# TASK_ARRIVAL_INTERVAL_SECONDS = 0.05  # 50ms average interval
# TASK_DEADLINE_SECONDS = 1.0           # 1000ms deadline (very relaxed)

# --- HIGH LOAD SCENARIO (Multi-sensor ADAS) ---
# Suitable for: dense task arrivals, batch scheduling benefits
# Simulates multiple sensors (cameras, radar, lidar) triggering simultaneously
TASK_ARRIVAL_INTERVAL_SECONDS = 0.005  # 5ms average interval (10x higher load)

# Deadline options:
# Option 1: Fixed deadline (simpler, good for initial testing)
TASK_DEADLINE_SECONDS = 0.03           # 30ms deadline (6x task interval, moderate pressure)

# Option 2: Random deadline (more realistic, uncomment to use)
# Will be implemented in environment as: random.uniform(0.02, 0.05)
# This creates task heterogeneity: urgent tasks (20ms) vs relaxed tasks (50ms)
TASK_DEADLINE_MODE = "fixed"           # "fixed" or "random"
TASK_DEADLINE_MIN = 0.02               # Min deadline for random mode (20ms)
TASK_DEADLINE_MAX = 0.05               # Max deadline for random mode (50ms)

# How much simulation time (in seconds) passes in a single step of the environment.
# This defines the time resolution of our simulation.
SIM_STEP_SECONDS = 0.01 # 10 milliseconds


# ================================================================
# SECTION 3: RL AGENT & TRAINING PARAMETERS (The "Coach's" Plan)
# ================================================================
# Total number of timesteps to train the agent for.
TOTAL_TIMESTEPS = 100_000

# Learning rate for the RL agent's neural network.
LEARNING_RATE = 0.0001

# Discount factor for future rewards. A value close to 1 means the agent
# cares more about long-term rewards.
GAMMA = 0.99

# The size of the replay buffer, where the agent stores past experiences.
BUFFER_SIZE = 10_000

# How many steps of experience to collect before starting to train the model.
LEARNING_STARTS = 1000


# ================================================================
# SECTION 4: ENVIRONMENT DEFINITION (The "Game" Rules)
# ================================================================
# The number of features in our state vector.
# [queue_length, time_to_nearest_deadline, time_since_oldest_task]
NUM_STATE_FEATURES = 3

# --- Action Space Definitions ---
# Batch-aware action space:
# Action 0: WAIT (do not dispatch)
# Action 1-7: Dispatch batch of size [1, 2, 4, 8, 16, 32, 64]
ACTION_WAIT = 0
BATCH_SIZE_OPTIONS = [1, 2, 4, 8, 16, 32, 64]
NUM_ACTIONS = len(BATCH_SIZE_OPTIONS) + 1  # +1 for WAIT action


# ================================================================
# SECTION 5: ADAS-SPECIFIC PARAMETERS
# ================================================================
# Dataset path for real data flow environment
IMAGENETTE_PATH = "data/imagenette2"

# Device for neural network inference (real environment only)
# NOTE: Set to "cpu" if you have GPU compatibility issues (e.g., RTX 50-series)
INFERENCE_DEVICE = "cuda"  # "cuda" for GPU, "cpu" for CPU-only

# Task arrival mode: "poisson" or "fixed_rate"
TASK_ARRIVAL_MODE = "poisson"  # Can be changed to "fixed_rate" for video stream simulation
FIXED_FRAME_RATE = 30  # FPS for fixed_rate mode (ADAS camera simulation)


# ================================================================
# SECTION 6: REWARD FUNCTION PARAMETERS
# ================================================================
# Reward for successfully completing a task before deadline
REWARD_TASK_SUCCESS = 2.0

# Penalty for missing a task's deadline
PENALTY_TASK_MISS = 10.0

# Penalty for a task expiring in the queue
PENALTY_QUEUE_EXPIRY = 15.0

# Latency penalty coefficient (to minimize overall system latency)
# --- For LOW LOAD: use 0.1 (less important)
# --- For HIGH LOAD: use 0.3 (more important to optimize latency)
LATENCY_PENALTY_COEFF = 0.3  # Increased for high-load scenario

# Batch efficiency bonus coefficient (encourage batching)
# --- For LOW LOAD: 0.5 may cause excessive waiting
# --- For HIGH LOAD: 0.5 is good (batching has real benefits)
BATCH_BONUS_COEFF = 0.5

# Invalid action penalties
PENALTY_EMPTY_QUEUE = 0.5
PENALTY_NODE_BUSY = 1.0
PENALTY_WAIT = 0.01  # Small penalty for waiting