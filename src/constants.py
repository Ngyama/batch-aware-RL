"""
Constants and Configuration File for the Batch-Aware RL Scheduler Simulation.

This file centralizes all the static parameters for the simulation,
environment, and RL agent, making it easy to tune and modify experiments.
"""

# ================================================================
# SECTION 1: PERFORMANCE PROFILE (The "Laws of Physics")
# ================================================================
# This dictionary was generated by profile_model.py.
# It defines the "physical law" of our Edge Node, specifying the
# processing time (in seconds) for a batch of a given size.
# Key: batch_size (int)
# Value: latency_in_seconds (float)
PERFORMANCE_PROFILE = {
    1: 0.0016378,
    2: 0.0015201,
    4: 0.0013424,
    8: 0.0016604,
    16: 0.0027963,
    32: 0.0053908,
    64: 0.0119798
}


# ================================================================
# SECTION 2: SIMULATION PARAMETERS (How the World Behaves)
# ================================================================
# Average time (in seconds) between the arrival of two consecutive tasks.
# For example, 0.05 means a new task arrives on average every 50ms.
# This simulates the load on the system. A smaller value means higher load.
TASK_ARRIVAL_INTERVAL_SECONDS = 0.05

# The lifespan (in seconds) of a task after it arrives.
# If a task is not completed by arrival_time + deadline, it is considered failed.
TASK_DEADLINE_SECONDS = 1.0

# How much simulation time (in seconds) passes in a single step of the environment.
# This defines the time resolution of our simulation.
SIM_STEP_SECONDS = 0.01 # 10 milliseconds


# ================================================================
# SECTION 3: RL AGENT & TRAINING PARAMETERS (The "Coach's" Plan)
# ================================================================
# Total number of timesteps to train the agent for.
TOTAL_TIMESTEPS = 100_000

# Learning rate for the RL agent's neural network.
LEARNING_RATE = 0.0001

# Discount factor for future rewards. A value close to 1 means the agent
# cares more about long-term rewards.
GAMMA = 0.99

# The size of the replay buffer, where the agent stores past experiences.
BUFFER_SIZE = 10_000

# How many steps of experience to collect before starting to train the model.
LEARNING_STARTS = 1000


# ================================================================
# SECTION 4: ENVIRONMENT DEFINITION (The "Game" Rules)
# ================================================================
# The number of features in our state vector.
# [queue_length, time_since_oldest_task_arrival, time_to_nearest_deadline, edge_node_status]
NUM_STATE_FEATURES = 4

# --- Action Space Definitions ---
# These constants make the code in environment.py more readable.
ACTION_WAIT = 0
ACTION_DISPATCH = 1